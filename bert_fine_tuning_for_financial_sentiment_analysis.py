# -*- coding: utf-8 -*-
"""BERT Fine-Tuning for Financial Sentiment Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F3Ux1CRzNI5zvlMF49cumd1BPwsRsogK
"""

!pip install transformers datasets torch
!pip install faiss-gpu transformers datasets

from IPython.display import clear_output
clear_output()
print("\nDone")

"""The dataset is loaded from the Hugging Face hub with the configuration 'sentences_allagree', which contains sentences with unanimous sentiment agreement by annotators."""

from datasets import load_dataset
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from transformers import DataCollatorWithPadding
import torch
from huggingface_hub import notebook_login
import numpy as np


# Step 1: Authenticate with Hugging Face
notebook_login('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')

# Load the dataset with a specific configuration
dataset = load_dataset('takala/financial_phrasebank', 'sentences_allagree', split='train')

"""The script uses the pre-trained BERT model (bert-base-uncased) and its corresponding tokenizer.
The model is set up for sequence classification with 3 output labels: positive, neutral, and negative.

A preprocessing function is defined to tokenize the sentences, truncate them to a maximum length of 128 tokens, and pad them as needed.
The dataset is tokenized using this function and split into training and validation sets with a 90-10 split.
"""

# Define the model and tokenizer
model_name = 'bert-base-uncased'
tokenizer = BertTokenizer.from_pretrained(model_name)
model = BertForSequenceClassification.from_pretrained(model_name, num_labels=3)  # 3 labels: positive, neutral, negative

"""Define a preprocessing function to tokenize the sentences

Apply the preprocessing function to the dataset
'batched=True' means the function processes multiple sentences at once for efficiency
"""

# Preprocess the data
def preprocess_function(examples):
    return tokenizer(examples['sentence'], truncation=True, padding='max_length', max_length=128)

tokenized_dataset = dataset.map(preprocess_function, batched=True)

"""Define a data collator to handle padding dynamically for batches during training"""

# Define data collator
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

"""Split the tokenized dataset into training and evaluation sets (90% train, 10% eval)"""

# Split dataset into training and validation sets
train_test_split = tokenized_dataset.train_test_split(test_size=0.1)
train_dataset = train_test_split['train']
eval_dataset = train_test_split['test']

"""Define training arguments specifying how the model should be trained"""

# Define training arguments
training_args = TrainingArguments(
    output_dir='./results',
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
    load_best_model_at_end=True,
)

"""Initialize the Trainer with the model, training arguments, datasets, tokenizer, and data collator

Train the model using the specified training arguments and datasets
"""

# Define trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
    data_collator=data_collator,
)

# Train the model
trainer.train()

"""Evaluate the model on the evaluation dataset and print the results"""

# Evaluate the model
eval_results = trainer.evaluate()
print(f"Evaluation results: {eval_results}")

"""Move the model to the appropriate device (GPU if available, otherwise CPU)"""

# Move the model to the appropriate device (CPU or GPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

"""Define a function to predict sentiment for a list of sentences"""

def predict_sentiment(sentences):
    # Ensure the input tensors are on the same device as the model
    inputs = tokenizer(sentences, return_tensors='pt', padding=True, truncation=True, max_length=128).to(device)
    outputs = model(**inputs)
    predictions = torch.argmax(outputs.logits, dim=-1).cpu().numpy()  # Move predictions to CPU for numpy compatibility

    # Map the predictions to sentiment labels
    sentiment_map = {0: "positive", 1: "neutral", 2: "negative"}
    results = [sentiment_map[pred] for pred in predictions]

    return results

"""
Example sentences to test the prediction function"""

# Example sentences to test the predictions
test_sentences = [
    "The company reported a significant increase in profits this quarter.",
    "The market reaction was mixed with no clear direction.",
    "The financial outlook is grim as losses continue to mount."
]

"""Predict sentiments for the example sentences

"""

# Predict sentiments
predicted_sentiments = predict_sentiment(test_sentences)
for sentence, sentiment in zip(test_sentences, predicted_sentiments):
    print(f"Sentence: '{sentence}' -> Sentiment: {sentiment}")

# Save the fine-tuned model
model.save_pretrained("./lora-finetuned-model")
tokenizer.save_pretrained("./lora-finetuned-model")